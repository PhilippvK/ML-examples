{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729ef4ca",
   "metadata": {},
   "source": [
    "# KWS Training Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659464b",
   "metadata": {},
   "source": [
    "blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb549695",
   "metadata": {},
   "source": [
    "## 0. Install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d34d8b",
   "metadata": {},
   "source": [
    "The following steps should ideally done before launching this Jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974d7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a9df",
   "metadata": {},
   "source": [
    "## 1. Python imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ef7a0",
   "metadata": {},
   "source": [
    "Python builtin dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9313e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e483",
   "metadata": {},
   "source": [
    "Third party dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8d8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 16:30:24.573611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-15 16:30:24.573626: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173d714",
   "metadata": {},
   "source": [
    "Helper scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d8c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import models\n",
    "from test_tflite import tflite_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a864db",
   "metadata": {},
   "source": [
    "## 2. Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3210afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = argparse.Namespace()  # TODO: use upper-case constants instead?\n",
    "\n",
    "# TODO: useful?\n",
    "FLAGS.model_name = \"kws_model\"\n",
    "\n",
    "# Location of speech training data archive on the web.\n",
    "FLAGS.data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "\n",
    "# Where to download the speech training data to.\n",
    "FLAGS.data_dir = '/tmp/speech_dataset/'\n",
    "\n",
    "# How loud the background noise should be, between 0 and 1.\n",
    "FLAGS.background_volume = 0.1\n",
    "\n",
    "# How many of the training samples have background noise mixed in.\n",
    "FLAGS.background_frequency = 0.8\n",
    "\n",
    "# How much of the training data should be silence.\n",
    "FLAGS.silence_percentage = 10.0\n",
    "\n",
    "# How much of the training data should be unknown words\n",
    "FLAGS.unknown_percentage = 10.0\n",
    "\n",
    "# Range to randomly shift the training audio by in time.\n",
    "FLAGS.time_shift_ms = 100.0\n",
    "\n",
    "# What percentage of wavs to use as a test set.\n",
    "FLAGS.testing_percentage = 10\n",
    "\n",
    "# What percentage of wavs to use as a validation set.\n",
    "FLAGS.validation_percentage = 10\n",
    "\n",
    "# Expected sample rate of the wavs\n",
    "FLAGS.sample_rate = 16000\n",
    "\n",
    "# Expected duration in milliseconds of the wavs\n",
    "FLAGS.clip_duration_ms = 1000\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_size_ms = 30.0\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_stride_ms = 10.0\n",
    "\n",
    "# How many bins to use for the MFCC fingerprint\n",
    "FLAGS.dct_coefficient_count = 40\n",
    "\n",
    "# How many training loops to run\n",
    "FLAGS.how_many_training_steps = '1500,300'\n",
    "\n",
    "# How often to evaluate the training results.\n",
    "FLAGS.eval_step_interval = 400\n",
    "\n",
    "# How large a learning rate to use when training.\n",
    "FLAGS.learning_rate = '0.001,0.0001'\n",
    "\n",
    "# How many items to train with at once\n",
    "FLAGS.batch_size = 100\n",
    "\n",
    "# Where to save summary logs for TensorBoard.\n",
    "FLAGS.summaries_dir = '/tmp/retrain_logs'\n",
    "\n",
    "# Words to use (others will be added to an unknown label)\n",
    "FLAGS.wanted_words = 'yes,no,up,down,left,right,on,off,stop,go'\n",
    "\n",
    "# Directory to write event logs and checkpoint.\n",
    "FLAGS.train_dir = '/tmp/speech_commands_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598c217",
   "metadata": {},
   "source": [
    "## 3. Create Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189906a",
   "metadata": {},
   "source": [
    "Define a model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2081bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_settings):\n",
    "    \"\"\"Builds a model with a single depthwise-convolution layer followed by a single fully-connected layer.\n",
    "    For details see https://arxiv.org/abs/1711.07128.\n",
    "    Args:\n",
    "        model_settings: Dict of different settings for model training.\n",
    "    Returns:\n",
    "        tf.keras Model of the 'CNN' architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get relevant model setting.\n",
    "    input_frequency_size = model_settings['dct_coefficient_count']\n",
    "    input_time_size = model_settings['spectrogram_length']\n",
    "\n",
    "    ### Task X: REPLACE CODE BELOW ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(model_settings[\"fingerprint_size\"]), name=\"input\")\n",
    "\n",
    "    # Reshape the flattened input.\n",
    "    x = tf.reshape(inputs, shape=(-1, input_time_size, input_frequency_size, 1))\n",
    "\n",
    "    # First convolution.\n",
    "    x = tf.keras.layers.DepthwiseConv2D(\n",
    "        depth_multiplier=8,\n",
    "        kernel_size=(10, 8),\n",
    "        strides=(2, 2),\n",
    "        padding=\"SAME\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "\n",
    "    # Flatten for fully connected layers.\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Output fully connected.\n",
    "    output = tf.keras.layers.Dense(units=model_settings[\"label_count\"], activation=\"softmax\")(x)\n",
    "    \n",
    "    ### Task X: REPLACE CODE ABOVE ###\n",
    "\n",
    "    return tf.keras.Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd90760",
   "metadata": {},
   "source": [
    "## 4. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c5339",
   "metadata": {},
   "source": [
    "Define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model_settings = models.prepare_model_settings(len(data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "                                                   FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "                                                   FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "\n",
    "    # Create the model.\n",
    "    model = create_model(model_settings)\n",
    "\n",
    "    audio_processor = data.AudioProcessor(data_url=FLAGS.data_url,\n",
    "                                          data_dir=FLAGS.data_dir,\n",
    "                                          silence_percentage=FLAGS.silence_percentage,\n",
    "                                          unknown_percentage=FLAGS.unknown_percentage,\n",
    "                                          wanted_words=FLAGS.wanted_words.split(','),\n",
    "                                          validation_percentage=FLAGS.validation_percentage,\n",
    "                                          testing_percentage=FLAGS.testing_percentage,\n",
    "                                          model_settings=model_settings)\n",
    "\n",
    "    # We decay learning rate in a constant piecewise way to help learning.\n",
    "    training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(',')))\n",
    "    learning_rates_list = list(map(float, FLAGS.learning_rate.split(',')))\n",
    "    lr_boundary_list = training_steps_list[:-1]  # Only need the values at which to change lr.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=lr_boundary_list,\n",
    "                                                                       values=learning_rates_list)\n",
    "\n",
    "    # Specify the optimizer configurations.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\t# Compile the model.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Prepare/split the dataset.\n",
    "    train_data = audio_processor.get_data(audio_processor.Modes.TRAINING,\n",
    "                                          FLAGS.background_frequency, FLAGS.background_volume,\n",
    "                                          int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000))\n",
    "    train_data = train_data.repeat().batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION)\n",
    "    val_data = val_data.batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # We train for a max number of iterations so need to calculate how many 'epochs' this will be.\n",
    "    training_steps_max = np.sum(training_steps_list)\n",
    "    training_epoch_max = int(np.ceil(training_steps_max / FLAGS.eval_step_interval))\n",
    "\n",
    "    # Callbacks.\n",
    "    train_dir = Path(FLAGS.train_dir) / \"best\"\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=(train_dir / (FLAGS.model_name + \"_{val_accuracy:.3f}_ckpt\")),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=FLAGS.summaries_dir)\n",
    "\n",
    "    # Train the model.\n",
    "    model.fit(x=train_data,\n",
    "              steps_per_epoch=FLAGS.eval_step_interval,\n",
    "              epochs=training_epoch_max,\n",
    "              validation_data=val_data,\n",
    "              callbacks=[model_checkpoint_callback, tensorboard_callback])\n",
    "\n",
    "    # Test and save the model.\n",
    "    test_data = audio_processor.get_data(audio_processor.Modes.TESTING)\n",
    "    test_data = test_data.batch(FLAGS.batch_size)\n",
    "\n",
    "    # Evaluate the model performace.\n",
    "    test_loss, test_acc = model.evaluate(x=test_data)\n",
    "    print(f'Final test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fc3e4",
   "metadata": {},
   "source": [
    "Invoke training procedure (**Warning:** This will take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29abfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa0ebd",
   "metadata": {},
   "source": [
    "Pick a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d8bfaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1662091861.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [6]\u001b[0;36m\u001b[0m\n\u001b[0;31m    FLAGS.checkpoint = ???  # TODO: pick best?\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "FLAGS.checkpoint = ???  # TODO: pick best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e88bbc",
   "metadata": {},
   "source": [
    "## 5. Test trained TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e1282",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edd49a",
   "metadata": {},
   "source": [
    "## 6. Quantization and Conversion to TFLite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a558a",
   "metadata": {},
   "source": [
    "Define conversion procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1e04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(model_settings, audio_processor, checkpoint, quantize, inference_type, tflite_path):\n",
    "    \"\"\"Load our trained floating point model and convert it.\n",
    "    TFLite conversion or post training quantization is performed and the\n",
    "    resulting model is saved as a TFLite file.\n",
    "    We use samples from the validation set to do post training quantization.\n",
    "    Args:\n",
    "        model_settings: Dictionary of common model settings.\n",
    "        audio_processor: Audio processor class object.\n",
    "        checkpoint: Path to training checkpoint to load.\n",
    "        quantize: Whether to quantize the model or convert to fp32 TFLite model.\n",
    "        inference_type: Input/output type of the quantized model.\n",
    "        tflite_path: Output TFLite file save path.\n",
    "    \"\"\"\n",
    "    model = models.create_model(model_settings, False)\n",
    "    model.load_weights(checkpoint).expect_partial()\n",
    "\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION).batch(1)\n",
    "\n",
    "    def _rep_dataset():  # TODO: make this a student task?\n",
    "        \"\"\"Generator function to produce representative dataset.\"\"\"\n",
    "        i = 0\n",
    "        for mfcc, label in val_data:\n",
    "            if i > NUM_REP_DATA_SAMPLES:\n",
    "                break\n",
    "            i += 1\n",
    "            yield [mfcc]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if quantize:\n",
    "        # Quantize model and save to disk.\n",
    "        if inference_type=='int8':\n",
    "            converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "\n",
    "        # Int8 post training quantization needs representative dataset.\n",
    "        converter.representative_dataset = _rep_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print('{} model saved to {}.'.format(\"Quantized\" if quantize else \"Converted\", tflite_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ce951",
   "metadata": {},
   "source": [
    "Prepare for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02325bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(len(data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "                                               FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "                                               FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "\n",
    "audio_processor = data.AudioProcessor(data_url=FLAGS.data_url,\n",
    "                                      data_dir=FLAGS.data_dir,\n",
    "                                      silence_percentage=FLAGS.silence_percentage,\n",
    "                                      unknown_percentage=FLAGS.unknown_percentage,\n",
    "                                      wanted_words=FLAGS.wanted_words.split(','),\n",
    "                                      validation_percentage=FLAGS.validation_percentage,\n",
    "                                      testing_percentage=FLAGS.testing_percentage,\n",
    "                                      model_settings=model_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f18f12",
   "metadata": {},
   "source": [
    "Invoke conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_path_quantized = f'{FLAGS.model_name}_quantized.tflite'\n",
    "tflite_path = f'{FLAGS.model_name}.tflite'\n",
    "\n",
    "# Load floating point model from checkpoint and convert it.\n",
    "convert(model_settings, audio_processor, FLAGS.checkpoint,\n",
    "        False, \"fp32\", tflite_path)\n",
    "\n",
    "# Quantize model from checkpoint and convert it.\n",
    "convert(model_settings, audio_processor, FLAGS.checkpoint,\n",
    "        True, \"int8\", tflite_path_quantized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553a6c0",
   "metadata": {},
   "source": [
    "## 7. Test Converted TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26094f4",
   "metadata": {},
   "source": [
    "Test the newly converted model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ea902",
   "metadata": {},
   "source": [
    "**Floating Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60725811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_test(model_settings, audio_processor, tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9106b4",
   "metadata": {},
   "source": [
    "**Quantized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb58095",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_test(model_settings, audio_processor, tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b568b",
   "metadata": {},
   "source": [
    "## 8. Visualize TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd240b9",
   "metadata": {},
   "source": [
    "TODO: Netron!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61ea1",
   "metadata": {},
   "source": [
    "## 9. Performance and Memory Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0977c",
   "metadata": {},
   "source": [
    "### 9.1 ROM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86706dd5",
   "metadata": {},
   "source": [
    "TFLITE filesize: Weights+Graph-Metadata(Tensors/Operators) -> ? kB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d6e0b",
   "metadata": {},
   "source": [
    "On-device:\n",
    "\n",
    "Weights -> ROM (Float(4B)/Quantized(1B))\n",
    "\n",
    "Graph -> Depends on inference engine\n",
    "Kernel implementations -> ROM\n",
    "\n",
    "* Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c180a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task X: REPLACE CODE BELOW ###\n",
    "\n",
    "### Task X: REPLACE CODE ABOVE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6456a0",
   "metadata": {},
   "source": [
    "### 9.2 RAM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7b2c",
   "metadata": {},
   "source": [
    "Memory Planning!\n",
    "\n",
    "Arena!\n",
    "\n",
    "Worst case: sum of all intermedia buffers\n",
    "\n",
    "inputs/outputs -> Depending on the \"inference engine\"\n",
    "\n",
    "For linear/sequential models: biggest in/out pair of a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd12112",
   "metadata": {},
   "source": [
    "### 9.3 Number of MAC Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8362b66",
   "metadata": {},
   "source": [
    "FLOPS vs MACS?\n",
    "\n",
    "https://stackoverflow.com/questions/56138754/formula-to-compute-the-number-of-macs-in-a-convolutional-neural-network\n",
    "\n",
    "https://leimao.github.io/blog/Depthwise-Separable-Convolution/ (pytorch)\n",
    "\n",
    "https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
    "\n",
    "https://cdmana.com/2021/04/20210413132349621b.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d\n",
    "\n",
    "(https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)?\n",
    "\n",
    "!!!https://machinethink.net/blog/how-fast-is-my-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871f06e",
   "metadata": {},
   "source": [
    "### 9.4 Constraints fulfilled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8eeac1",
   "metadata": {},
   "source": [
    "Example target: ESP32C3\n",
    "    \n",
    "Warning: drivers etc sensor stuff will contribute to equations as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ed25b",
   "metadata": {},
   "source": [
    "## Further considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f960054",
   "metadata": {},
   "source": [
    "Optimize for size?\n",
    "\n",
    "Model compression: Pruning\n",
    "\n",
    "Packing (Quantization-aware training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865a744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
