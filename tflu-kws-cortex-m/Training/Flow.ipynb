{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729ef4ca",
   "metadata": {},
   "source": [
    "# KWS Training Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659464b",
   "metadata": {},
   "source": [
    "TODO: blabla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb549695",
   "metadata": {},
   "source": [
    "## 0. Install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d34d8b",
   "metadata": {},
   "source": [
    "The following steps should ideally done before launching this Jupyter notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a576794",
   "metadata": {},
   "source": [
    "**1. Clone repository**\n",
    "\n",
    "```\n",
    "git clone ???\n",
    "```\n",
    "\n",
    "**2. Create virtual python environment**\n",
    "\n",
    "```\n",
    "virtualenv -p python3.8 .venv\n",
    "```\n",
    "\n",
    "**3. Enter virtual python environment**\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "**4. Install python packages into environment**\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**5. Start jupyter notebook**\n",
    "    \n",
    "```\n",
    "jupyter ntebook Flow.ipynb\n",
    "```\n",
    "\n",
    "  If using a remote host, append: ` --no-browser --ip 0.0.0.0 --port XXXX` (where XXXX > 1000)\n",
    "  \n",
    "  If you experience warnings it might help to use ` --NotebookApp.iopub_msg_rate_limit=1.0e10`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a9df",
   "metadata": {},
   "source": [
    "## 1. Python imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ef7a0",
   "metadata": {},
   "source": [
    "Python builtin dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9313e69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Reduce verbosity\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e483",
   "metadata": {},
   "source": [
    "Third party dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d8efb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.get_logger().setLevel('WARN')  # Reduce verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dd0c5",
   "metadata": {},
   "source": [
    "Jupyter specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c356b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173d714",
   "metadata": {},
   "source": [
    "Helper scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8c8ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import models\n",
    "from test import get_val_accuracy, get_test_accuracy\n",
    "from test_tflite import tflite_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a864db",
   "metadata": {},
   "source": [
    "## 2. Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3210afe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FLAGS = argparse.Namespace()  # TODO: use upper-case constants instead?\n",
    "\n",
    "# TODO: useful?\n",
    "FLAGS.model_name = \"kws_model\"\n",
    "\n",
    "# Location of speech training data archive on the web.\n",
    "FLAGS.data_url = 'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "\n",
    "# Where to download the speech training data to.\n",
    "FLAGS.data_dir = os.path.join(\"/tmp\", os.getlogin(), \"speech_dataset\")\n",
    "\n",
    "# How loud the background noise should be, between 0 and 1.\n",
    "FLAGS.background_volume = 0.1\n",
    "\n",
    "# How many of the training samples have background noise mixed in.\n",
    "FLAGS.background_frequency = 0.8\n",
    "\n",
    "# How much of the training data should be silence.\n",
    "FLAGS.silence_percentage = 10.0\n",
    "\n",
    "# How much of the training data should be unknown words\n",
    "FLAGS.unknown_percentage = 10.0\n",
    "\n",
    "# Range to randomly shift the training audio by in time.\n",
    "FLAGS.time_shift_ms = 100.0\n",
    "\n",
    "# What percentage of wavs to use as a test set.\n",
    "FLAGS.testing_percentage = 10\n",
    "\n",
    "# What percentage of wavs to use as a validation set.\n",
    "FLAGS.validation_percentage = 10\n",
    "\n",
    "# Expected sample rate of the wavs\n",
    "FLAGS.sample_rate = 16000\n",
    "\n",
    "# Expected duration in milliseconds of the wavs\n",
    "FLAGS.clip_duration_ms = 1000\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_size_ms = 30.0\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_stride_ms = 20.0\n",
    "\n",
    "# How many bins to use for the MFCC fingerprint\n",
    "FLAGS.dct_coefficient_count = 40\n",
    "\n",
    "# How many training loops to run\n",
    "FLAGS.how_many_training_steps = \"1500,300\"  # 12000,3000\n",
    "\n",
    "# How often to evaluate the training results.\n",
    "FLAGS.eval_step_interval = 400\n",
    "\n",
    "# How large a learning rate to use when training.\n",
    "FLAGS.learning_rate = \"0.001,0.0001\"\n",
    "\n",
    "# How many items to train with at once\n",
    "FLAGS.batch_size = 100\n",
    "\n",
    "# Where to save summary logs for TensorBoard.\n",
    "# FLAGS.summaries_dir = '/tmp/retrain_logs'\n",
    "\n",
    "# Words to use (others will be added to an unknown label)\n",
    "FLAGS.wanted_words = \"yes,no,one,two\"\n",
    "\n",
    "# Directory to write event logs and checkpoint.\n",
    "FLAGS.train_dir = os.path.join(\"/tmp\", os.getlogin(), \"speech_commands_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598c217",
   "metadata": {},
   "source": [
    "## 3. Create Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f94f40",
   "metadata": {},
   "source": [
    "Get model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91604a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(len(data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "                                               FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "                                               FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189906a",
   "metadata": {},
   "source": [
    "Define a model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2081bfaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(model_settings):\n",
    "    \"\"\"Builds a model with a single depthwise-convolution layer followed by a single fully-connected layer.\n",
    "    Args:\n",
    "        model_settings: Dict of different settings for model training.\n",
    "    Returns:\n",
    "        tf.keras Model of the 'CNN' architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get relevant model setting.\n",
    "    input_frequency_size = model_settings['dct_coefficient_count']\n",
    "    input_time_size = model_settings['spectrogram_length']\n",
    "\n",
    "    ### Task X: REPLACE CODE BELOW ###\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(model_settings[\"fingerprint_size\"]), name=\"input\")\n",
    "\n",
    "    # Reshape the flattened input.\n",
    "    x = tf.reshape(inputs, shape=(-1, input_time_size, input_frequency_size, 1))\n",
    "\n",
    "    # First convolution.\n",
    "    x = tf.keras.layers.DepthwiseConv2D(\n",
    "        depth_multiplier=8,\n",
    "        kernel_size=(10, 8),\n",
    "        strides=(2, 2),\n",
    "        padding=\"SAME\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "\n",
    "    # Flatten for fully connected layers.\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Output fully connected.\n",
    "    output = tf.keras.layers.Dense(units=model_settings[\"label_count\"], activation=\"softmax\")(x)\n",
    "    \n",
    "    ### Task X: REPLACE CODE ABOVE ###\n",
    "\n",
    "    return tf.keras.Model(inputs, output, name=FLAGS.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278841d",
   "metadata": {},
   "source": [
    "Generate keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec55b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(model_settings)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752dda8",
   "metadata": {},
   "source": [
    "## 4. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processor = data.AudioProcessor(data_url=FLAGS.data_url,\n",
    "                                      data_dir=FLAGS.data_dir,\n",
    "                                      silence_percentage=FLAGS.silence_percentage,\n",
    "                                      unknown_percentage=FLAGS.unknown_percentage,\n",
    "                                      wanted_words=FLAGS.wanted_words.split(','),\n",
    "                                      validation_percentage=FLAGS.validation_percentage,\n",
    "                                      testing_percentage=FLAGS.testing_percentage,\n",
    "                                      model_settings=model_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd90760",
   "metadata": {},
   "source": [
    "## 5. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c5339",
   "metadata": {},
   "source": [
    "Define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2f34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model, audio_processor):\n",
    "    # We decay learning rate in a constant piecewise way to help learning.\n",
    "    training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(',')))\n",
    "    learning_rates_list = list(map(float, FLAGS.learning_rate.split(',')))\n",
    "    lr_boundary_list = training_steps_list[:-1]  # Only need the values at which to change lr.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=lr_boundary_list,\n",
    "                                                                       values=learning_rates_list)\n",
    "\n",
    "    # Specify the optimizer configurations.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\t# Compile the model.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Prepare/split the dataset.\n",
    "    train_data = audio_processor.get_data(audio_processor.Modes.TRAINING,\n",
    "                                          FLAGS.background_frequency, FLAGS.background_volume,\n",
    "                                          int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000))\n",
    "    train_data = train_data.repeat().batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION)\n",
    "    val_data = val_data.batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # We train for a max number of iterations so need to calculate how many 'epochs' this will be.\n",
    "    training_steps_max = np.sum(training_steps_list)\n",
    "    training_epoch_max = int(np.ceil(training_steps_max / FLAGS.eval_step_interval))\n",
    "\n",
    "    # Callbacks.\n",
    "    train_dir = Path(FLAGS.train_dir) / \"best\"\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=(train_dir / (FLAGS.model_name + \"_{val_accuracy:.3f}_ckpt\")),\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Train the model.\n",
    "    model.fit(x=train_data,\n",
    "              steps_per_epoch=FLAGS.eval_step_interval,\n",
    "              epochs=training_epoch_max,\n",
    "              validation_data=val_data,\n",
    "              callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # Test and save the model.\n",
    "    test_data = audio_processor.get_data(audio_processor.Modes.TESTING)\n",
    "    test_data = test_data.batch(FLAGS.batch_size)\n",
    "\n",
    "    # Evaluate the model performace.\n",
    "    test_loss, test_acc = model.evaluate(x=test_data)\n",
    "    print(f'Final test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fc3e4",
   "metadata": {},
   "source": [
    "Invoke training procedure (**Warning:** This will take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, audio_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0cb48",
   "metadata": {},
   "source": [
    "Determine latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(Path(FLAGS.train_dir) / \"best\")\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa0ebd",
   "metadata": {},
   "source": [
    "Pick a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8bfaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FLAGS.checkpoint = latest  # Feel free to choose a different one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e88bbc",
   "metadata": {},
   "source": [
    "## 6. Test trained TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e1282",
   "metadata": {},
   "source": [
    "Define test procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d71f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"Calculate accuracy and confusion matrices on validation and test sets.\n",
    "\n",
    "    Model is created and weights loaded from supplied command line arguments.\n",
    "    \"\"\"\n",
    "    model_settings = models.prepare_model_settings(len(data.prepare_words_list(FLAGS.wanted_words.split(','))),\n",
    "                                                   FLAGS.sample_rate, FLAGS.clip_duration_ms, FLAGS.window_size_ms,\n",
    "                                                   FLAGS.window_stride_ms, FLAGS.dct_coefficient_count)\n",
    "\n",
    "    # Create the model.\n",
    "    model = create_model(model_settings)\n",
    "\n",
    "    audio_processor = data.AudioProcessor(data_url=FLAGS.data_url,\n",
    "                                          data_dir=FLAGS.data_dir,\n",
    "                                          silence_percentage=FLAGS.silence_percentage,\n",
    "                                          unknown_percentage=FLAGS.unknown_percentage,\n",
    "                                          wanted_words=FLAGS.wanted_words.split(','),\n",
    "                                          validation_percentage=FLAGS.validation_percentage,\n",
    "                                          testing_percentage=FLAGS.testing_percentage,\n",
    "                                          model_settings=model_settings)\n",
    "\n",
    "    model.load_weights(FLAGS.checkpoint).expect_partial()\n",
    "\n",
    "    print(\"Running testing on validation set...\")\n",
    "    get_val_accuracy(model_settings, model, audio_processor, FLAGS.batch_size)\n",
    "    print()\n",
    "    print(\"Running testing on test set...\")\n",
    "    get_test_accuracy(model_settings, model, audio_processor, FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac3a38",
   "metadata": {},
   "source": [
    "Run test procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a52a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edd49a",
   "metadata": {},
   "source": [
    "## 7. Quantization and Conversion to TFLite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a558a",
   "metadata": {},
   "source": [
    "Define conversion procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e04f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_REP_DATA_SAMPLES = 100\n",
    "\n",
    "def convert(model, audio_processor, checkpoint, quantize, inference_type, tflite_path):\n",
    "    \"\"\"Load our trained floating point model and convert it.\n",
    "    TFLite conversion or post training quantization is performed and the\n",
    "    resulting model is saved as a TFLite file.\n",
    "    We use samples from the validation set to do post training quantization.\n",
    "    Args:\n",
    "        model: The keras model.\n",
    "        audio_processor: Audio processor class object.\n",
    "        checkpoint: Path to training checkpoint to load.\n",
    "        quantize: Whether to quantize the model or convert to fp32 TFLite model.\n",
    "        inference_type: Input/output type of the quantized model.\n",
    "        tflite_path: Output TFLite file save path.\n",
    "    \"\"\"\n",
    "    model.load_weights(checkpoint).expect_partial()\n",
    "\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION).batch(1)\n",
    "\n",
    "    def _rep_dataset():  # TODO: make this a student task?\n",
    "        \"\"\"Generator function to produce representative dataset.\"\"\"\n",
    "        i = 0\n",
    "        for mfcc, label in val_data:\n",
    "            if i > NUM_REP_DATA_SAMPLES:\n",
    "                break\n",
    "            i += 1\n",
    "            yield [mfcc]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if quantize:\n",
    "        # Quantize model and save to disk.\n",
    "        if inference_type=='int8':\n",
    "            converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "\n",
    "        # Int8 post training quantization needs representative dataset.\n",
    "        converter.representative_dataset = _rep_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print('{} model saved to {}.'.format(\"Quantized\" if quantize else \"Converted\", tflite_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f18f12",
   "metadata": {},
   "source": [
    "Invoke conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c5c4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tflite_path_quantized = f'{FLAGS.model_name}_quantized.tflite'\n",
    "tflite_path = f'{FLAGS.model_name}.tflite'\n",
    "\n",
    "# Load floating point model from checkpoint and convert it.\n",
    "convert(model, audio_processor, FLAGS.checkpoint,\n",
    "        False, \"fp32\", tflite_path)\n",
    "\n",
    "# Quantize model from checkpoint and convert it.\n",
    "convert(model, audio_processor, FLAGS.checkpoint,\n",
    "        True, \"int8\", tflite_path_quantized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553a6c0",
   "metadata": {},
   "source": [
    "## 8. Test Converted TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26094f4",
   "metadata": {},
   "source": [
    "Test the newly converted model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ea902",
   "metadata": {},
   "source": [
    "**Floating Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60725811",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tflite_test(model_settings, audio_processor, tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9106b4",
   "metadata": {},
   "source": [
    "**Quantized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb58095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tflite_test(model_settings, audio_processor, tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b568b",
   "metadata": {},
   "source": [
    "## 9. Visualize TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9117c52",
   "metadata": {},
   "source": [
    "**Figure 1:** Example Keras KWS Model\n",
    "<img src=\"basic_micro_speech_graph.png\" alt=\"drawing\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3faa45e",
   "metadata": {},
   "source": [
    "Use the following links to download the generated `.tflite` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1ef17",
   "metadata": {},
   "source": [
    "**Floating Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9a8be",
   "metadata": {},
   "source": [
    "**Quantized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a700cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac432529",
   "metadata": {},
   "source": [
    "Use the web application https://netron.app/ to generate a graph representation of the converted model.\n",
    "\n",
    "**Task X:** Save the resulting graph as PNG-image. (TODO: ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61ea1",
   "metadata": {},
   "source": [
    "## 10. Performance and Memory Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0977c",
   "metadata": {},
   "source": [
    "### 10.1 ROM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86706dd5",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "TFLITE filesize: Weights+Graph-Metadata(Tensors/Operators) -> ? kB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d6e0b",
   "metadata": {},
   "source": [
    "On-device:\n",
    "\n",
    "TODO\n",
    "\n",
    "Weights -> ROM (Float(4B)/Quantized(1B))\n",
    "\n",
    "Graph -> Depends on inference engine\n",
    "Kernel implementations -> ROM\n",
    "\n",
    "Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c6dca",
   "metadata": {},
   "source": [
    "**Task X:** Estimate the memory requirement to store all trained weights of the quantized model in ROM considering the used data types.\n",
    "\n",
    "*Hints:*\n",
    "- The `summary()` method of a keras model can be used to extract the number of constants used by an operator.\n",
    "- The dimension of the weight tensors can be also found in the Netron graph generated in the previous step.\n",
    "- Memory alignment requirements can be ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc95f",
   "metadata": {},
   "source": [
    "**<font color='red'>Solution X:</font>**\n",
    "\n",
    "All constant dimensions and their datatypes:\n",
    "\n",
    "1. DepthwiseConv2D filter/weights: `[1,10,8,8]` (int8)\n",
    "2. DepthwiseConv2D bias: `[8]` (int32)\n",
    "3. FullyConnected filter/weights: `[6,4000]` (int8)\n",
    "4. FullyConnected bias: `[6]` (int32)\n",
    "\n",
    "Total sum: `(1*10*8*8 + 6*4000) * 1 byte + (8+6) * 4 byte = 24696 byte` -> ~24.5 kB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc115913",
   "metadata": {},
   "source": [
    "**Task X:** Briefly explain the following model compression techniques in 2-3 sentences each:\n",
    "  - Sparsity\n",
    "  - Sub-byte quantization / \"Packing\"\n",
    "\n",
    "*Hints:*\n",
    "- At which level the technique has to be integrated into the training routine?\n",
    "- How is model inference affected by the compression of weights?\n",
    "- TFLite uses int32 for biases even if int8 quantization is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a58ecf",
   "metadata": {},
   "source": [
    "**<font color='red'>Solution X:</font>**\n",
    "\n",
    "- **Sparsity**\n",
    "\n",
    " TODO\n",
    " \n",
    " \n",
    "- **Sub-byte quantization**\n",
    "\n",
    "  TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6456a0",
   "metadata": {},
   "source": [
    "### 10.2 RAM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7b2c",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Memory Planning!\n",
    "\n",
    "Arena!\n",
    "\n",
    "Worst case: sum of all intermedia buffers\n",
    "\n",
    "inputs/outputs -> Depending on the \"inference engine\"\n",
    "\n",
    "For linear/sequential models: biggest in/out pair of a layer\n",
    "\n",
    "**Task X:** Estimate the dynamic memory requirement of the quantized model based on the TFLite graph only considering intermediate tensor buffers stored in RAM for the following memory-planning schemes:\n",
    "  - Worst case: no memory planning (no intermediate buffers/tensors share the same memory)\n",
    "  - Best case: optimal memory planning (e.g. found using an ILP-solver)\n",
    "\n",
    "*Assumptions:*\n",
    "- Neither branches nor nodes with multiple inputs/outputs extist in the trained model.\n",
    "- Assume that the graph is processed in a linear way so that at most 2 buffers will be used at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069926a0",
   "metadata": {},
   "source": [
    "**<font color='red'>Solution X:</font>**\n",
    "\n",
    "Intermediate buffers:\n",
    "\n",
    "1. `[1,49,40,1]`\n",
    "2. `[1,25,20,8]`\n",
    "3. `[1,4]`\n",
    "\n",
    "Largest combination of input and output buffer: 1. + 2. leads to `1*49*40*1 + 1*25*20*8 = 5960` elements which are each 1 Byte large -> ~6 kB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd12112",
   "metadata": {},
   "source": [
    "### 10.3 Number of MAC Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8362b66",
   "metadata": {},
   "source": [
    "In this section the compute demand of a given TFLite model should be estimated.\n",
    "\n",
    "As a first simplification we will only consider the operation which will have the biggest impact on the actual inference time: Multiply-Add (-> MAC)\n",
    "\n",
    "These operations can be found in Dense (FullyConnected), and convolutional layers. Thus other operations (here: Reshape, Flatten as well as activation functions) can be neglected for the following task.\n",
    "\n",
    "First, a formular to describe the number of MAC operations of the three major types of with repect to the given tensor dimensions and parameters.\n",
    "\n",
    "**Example (Dense/FullyConnected):**\n",
    "\n",
    "  Assume: `h_out=h_in`, `w_out=w_kernel`\n",
    "\n",
    "  `num_mac = h_out * w_out * h_kernel`\n",
    "  \n",
    "  For the example keras model: `1 * 6 * 4000` -> ~24k MACs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbe0b0",
   "metadata": {},
   "source": [
    "**Task X:** Estimate the number of Multiply-Add operations used in the quantized model (see Figure 1) by deriving a formula for `num_mac` in a convolutional layer with respect to `h_kernel`, `w_kernel`, `c_in(=1)`, `c_out`, `h_out`, `w_out`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccabb3a",
   "metadata": {},
   "source": [
    "**<font color='red'>Solution X:</font>**\n",
    "\n",
    "Assumption: no dilation!\n",
    "\n",
    "Formulas per layer:\n",
    "\n",
    "1. **FullyConnected**\n",
    "\n",
    "  See above!\n",
    "  \n",
    "\n",
    "2. **Conv2D**/**DepthwiseConv2D**\n",
    "\n",
    "  Output size is given, thus padding/stride/dilation can be ignored.\n",
    "  \n",
    "  Same formula for Conv2D and DepthwiseConv2D as long as depth_multiplier is considered in `c_out`! (TODO: double check, i.e. in TFLM codebase)\n",
    "\n",
    "  `num_mac = (h_kernel * w_kernel) * c_in * (h_out * w_out * c_out)`\n",
    "  \n",
    "  For **DepthwiseConv2D** example keras model: `(10 * 8) * 1 * (25 * 20 * 8) = 320000` -> ~320K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79185f20",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 11. Final challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93479ba",
   "metadata": {},
   "source": [
    "To pass the lab you have to design a model architecture for the keyword-spotting task which satisfies each of the following constraints:\n",
    "\n",
    "1. Accuracy of the quantized TFLite model is at least 90%\n",
    "2. Total memory requirement to store all constants/weights in ROM is at most 75kB (See Task X?) TODO: TFLITE file size instead?, update limit\n",
    "3. Best case memory requirement for intermediate tensors in RAM is at most 100kB (See Task X?) TODO: update limit\n",
    "4. Estimated number of MAC operations is at most 100000? (See Task X?) TODO: update limit\n",
    "\n",
    "\n",
    "*Hint:*\n",
    "- Training- or Model-parameters may not be changed to complete this challenge.\n",
    "- A subset of the following Keras layers should suffice: Dense, Conv2D, DepthwiseConv2D, Flatten, Reshape\n",
    "- Try to keep the total number of (despthwise)-convolutions and dense-layers below 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d104dc7",
   "metadata": {},
   "source": [
    "## Further information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5be654",
   "metadata": {},
   "source": [
    "Summary of tasks in this lab:\n",
    "\n",
    "- **Task 1:**\n",
    "- **Task 2:**\n",
    "- **Task 3:**\n",
    "- ...\n",
    "- **Task X:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe26b4a",
   "metadata": {},
   "source": [
    "TODO: decide if the estimination tasks should be solved using the example model or the final one?\n",
    "\n",
    "TODO: Should answers to theory questions be submitted or only be useful as \"Exam\"-Prep?\n",
    "\n",
    "TODO: Submit only model `.tflite` file (accuracy could be tested via CI) or also keras code?\n",
    "\n",
    "TODO: FLOPS vs MACS?\n",
    "\n",
    "TODO: useful links:\n",
    "https://stackoverflow.com/questions/56138754/formula-to-compute-the-number-of-macs-in-a-convolutional-neural-network\n",
    "\n",
    "https://leimao.github.io/blog/Depthwise-Separable-Convolution/ (pytorch)\n",
    "\n",
    "https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec\n",
    "\n",
    "https://cdmana.com/2021/04/20210413132349621b.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d\n",
    "\n",
    "(https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)?\n",
    "\n",
    "!!!https://machinethink.net/blog/how-fast-is-my-model/\n",
    "\n",
    "TODO: Conv output size for manual?\n",
    "\n",
    "**Conv2D**\n",
    "\n",
    "  General: `num_mac = (n_h - k_h + p_h + 1) x (n_w - k_w + p_w + 1)`\n",
    "  \n",
    "  With non-default stride: `num_mac = floor((n_h - k_h + p_h + s_h)/s_h) x floor((n_w - k_w + p_w + s_w)/s_w)`\n",
    "  \n",
    "  For `padding=\"VALID\"` (`p_h=p_w=0`): `num_mac = (n_h - k_h + p_h + 1) x (n_w - k_w + p_w + 1)`\n",
    "  \n",
    "  For `padding=\"SAME\"` (`p_h=k_h-1`, `p_w=k_w-1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
